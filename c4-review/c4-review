#!/usr/bin/env python3

import sys
import json
import argparse
import datetime
import os.path
import requests
import webbrowser
import concurrent.futures

c4auth_login = os.getenv("C4AUTH_LOGIN")
if not c4auth_login:
  print("C4AUTH_LOGIN environment variable not found.")
  print("Grab yours by:")
  print("- logging in on the C4 website")
  print("- reading it via a cookie editor i.e. the 'Edit this cookie' Chrome extension")
  print("- and copying it to the C4AUTH_LOGIN environment variable")
  exit(1)

BASE = 0.85 # Base for Sybil protection. Was once 0.9 but 0.85 since Jul 2024

parser = argparse.ArgumentParser(prog="c4-review", formatter_class=argparse.RawTextHelpFormatter,
                                 description="Analyzes the C4 findings data and provides stats. Estimates payout if you provide a handle")
subp = parser.add_subparsers(dest="command") # dest="command" means that we see which command was parsed

base_help = f"Change base (default {BASE}) for Sybil protection e.g. --base=0.9 (for old comps)"

payout = subp.add_parser("payouts", description="Find out the fraction of total pot or payouts")
payout.add_argument('contest_slug', type=str)
payout.add_argument('pot_size', type=int, nargs='?')
payout.add_argument('-d', '--ignore_disputed', type=bool)
payout.add_argument('-w', '--handle', type=str, nargs='?')
payout.add_argument('-b', '--base', type=str, nargs=1, help=base_help)

open = subp.add_parser("open", description="Opens all findings from a given warden in browser")
open.add_argument('contest_slug', type=str)
open.add_argument('handle', type=str)

def get_findings(ns):
  headers = {
    "content-type": "application/json",
    "cookie": f"C4AUTH-LOGIN={os.getenv('C4AUTH_LOGIN')}"
  }

  all_submissions = []

  def fetch_page(session, page):
    url = f"https://code4rena.com/api/v1/audits/{ns.contest_slug}/submissions?perPage=100&page={page}"
    resp = session.get(url, headers=headers)
    if resp.status_code != 200:
      try:
        body = resp.json()
      except Exception:
        body = resp.text
      raise RuntimeError(f"HTTP {resp.status_code} page {page}: {body}")
    try:
      response = resp.json()
    except Exception as e:
      raise RuntimeError(f"Invalid JSON on page {page}: {e} :: {resp.text[:500]}")
    if not isinstance(response, dict) or 'data' not in response:
      raise RuntimeError(f"Unexpected API response on page {page}: {response}")
    subs = response.get('data', {}).get('submissions', response.get('data'))
    if not isinstance(subs, list):
      subs = []
    pagination = response.get('pagination', {})
    return subs, pagination

  with requests.Session() as session:
    # First page (to get lastPage)
    try:
      subs, pagination = fetch_page(session, 1)
    except Exception as e:
      print(f"Error fetching findings: {e}")
      print("Hint: Ensure C4AUTH_LOGIN is valid and the contest is accessible.")
      exit(1)
    all_submissions.extend(subs)
    last_page = pagination.get('lastPage', 1) or 1

    # Remaining pages concurrently
    if last_page > 1:
      pages = list(range(2, last_page + 1))
      with concurrent.futures.ThreadPoolExecutor(max_workers=min(8, len(pages))) as ex:
        futures = {ex.submit(fetch_page, session, p): p for p in pages}
        for fut in concurrent.futures.as_completed(futures):
          p = futures[fut]
          try:
            subs, _ = fut.result()
            all_submissions.extend(subs)
          except Exception as e:
            print(f"Error fetching page {p}: {e}")
            exit(1)

  # Transform submissions list into finding-like aggregates used by the rest of the script
  by_finding = {}
  for sub in all_submissions:
    if isinstance(sub, dict) and 'finding' in sub and isinstance(sub['finding'], dict):
      finding_key = sub['finding'].get('uid') or sub['finding'].get('number')
      finding_number = sub['finding'].get('number')
      finding_uid = sub['finding'].get('uid')
    else:
      finding_key = (sub.get('title'), sub.get('severity'))
      finding_number = None
      finding_uid = None

    if finding_key not in by_finding:
      by_finding[finding_key] = {
        'uid': finding_uid,
        'number': finding_number,
        'evaluations': [],
        'submissions': { 'data': [] },
        'duplicates': 0,
      }

    for ev in (sub.get('evaluations') or []):
      if ev.get('type') in ['validity', 'severity']:
        by_finding[finding_key]['evaluations'].append(ev)

    by_finding[finding_key]['submissions']['data'].append({
      'number': sub.get('number'),
      'user': sub.get('user'),
      'team': sub.get('team'),
      'severity': sub.get('severity'),
      'evaluations': sub.get('evaluations') or [],
      'isPrimary': sub.get('isPrimary', False),
    })

  findings_list = []
  for f in by_finding.values():
    # Ensure primary submission, when present, is first to preserve business logic
    f['submissions']['data'].sort(key=lambda s: (not s.get('isPrimary', False), s.get('number') or 0))
    dups = 0
    for sub in f['submissions']['data']:
      credit = 1.0
      for ev in sub['evaluations']:
        if ev.get('type') == 'credit':
          try:
            credit = int(ev['value'][:-1]) / 100
          except Exception:
            credit = 1.0
      if credit > 0:
        dups += credit
    if dups == 0:
      dups = len(f['submissions']['data'])
    f['dups'] = dups
    findings_list.append(f)

  return findings_list

def pp_usd(n):
  return '${:0,.2f}'.format(round(n, 2))

def is_high(finding_or_submission):
  return finding_or_submission["severity"].lower() == "high"

def get_credit(submission):
  quality = None
  credit = 1.0

  for evaluation in submission["evaluations"]:
    if evaluation["type"] == "quality":
      quality = evaluation["value"]
    elif evaluation["type"] == "credit":
      credit = int(evaluation["value"][:-1]) / 100

  if quality != "sufficient":
    return 0

  return credit

def get_validity_and_severity(finding):
  validity = None
  severity = finding["submissions"]["data"][0]["severity"]

  for evaluation in finding["evaluations"]:
    if evaluation["type"] == "validity" and evaluation["userAuditRole"] != "sponsor":
      validity = evaluation["value"]
    elif evaluation["type"] == "severity":
      severity = evaluation["value"]

  return (validity, severity)

def open_cmd(ns):
  headers = {
    "content-type": "application/json",
    "cookie": f"C4AUTH-LOGIN={os.getenv('C4AUTH_LOGIN')}"
  }

  handle = ns.handle
  urls = []

  def fetch_page(session, page):
    url = f"https://code4rena.com/api/v1/audits/{ns.contest_slug}/submissions?perPage=100&page={page}"
    resp = session.get(url, headers=headers)
    if resp.status_code != 200:
      try:
        body = resp.json()
      except Exception:
        body = resp.text
      raise RuntimeError(f"HTTP {resp.status_code} page {page}: {body}")
    data = resp.json()
    subs = data.get('data', {}).get('submissions', data.get('data', []))
    if not isinstance(subs, list):
      subs = []
    pagination = data.get('pagination', {})
    return subs, pagination

  def matches_handle(sub):
    u = sub.get('user') or {}
    if u.get('handle') == handle:
      return True
    team = sub.get('team') or {}
    if team.get('handle') == handle:
      return True
    members = team.get('members') or []
    for m in members:
      if m.get('handle') == handle:
        return True
    return False

  with requests.Session() as session:
    try:
      subs, pagination = fetch_page(session, 1)
    except Exception as e:
      print(f"Error fetching submissions: {e}")
      exit(1)
    for s in subs:
      if matches_handle(s):
        urls.append(f"https://code4rena.com/contests/{ns.contest_slug}/submissions/{s.get('number')}")
    last_page = pagination.get('lastPage', 1) or 1
    if last_page > 1:
      pages = list(range(2, last_page + 1))
      with concurrent.futures.ThreadPoolExecutor(max_workers=min(8, len(pages))) as ex:
        futures = {ex.submit(fetch_page, session, p): p for p in pages}
        for fut in concurrent.futures.as_completed(futures):
          p = futures[fut]
          try:
            subs, _ = fut.result()
            for s in subs:
              if matches_handle(s):
                urls.append(f"https://code4rena.com/contests/{ns.contest_slug}/submissions/{s.get('number')}")
          except Exception as e:
            print(f"Error fetching page {p}: {e}")
            exit(1)

  # Open in browser
  for url in urls:
    webbrowser.open(url)

  return { "opened": len(urls), "urls": urls }

def payout(ns):
  findings = get_findings(ns)

  ws = {}
  total_shares = 0

  for finding in findings:
    num_dups = 0
    validity, severity = get_validity_and_severity(finding)
    finding["severity"] = severity

    if validity != "valid" or severity not in ["high", "medium"]:
      continue

    # Deduplicate submissions per participant (team or user) by max would-be sliceCredit
    # Build candidate list with computed would-be sliceCredit
    candidates = []
    for sub in finding["submissions"]["data"]:
      credit = get_credit(sub)
      if credit == 0:
        continue
      is_lead = bool(sub.get("isPrimary", False))
      slice_candidate = 1.3 if is_lead else credit
      # Participant key (team preferred)
      w = (sub.get("user") or {}).get("handle")
      team = sub.get("team")
      if team is not None:
        members = ", ".join(sorted(m.get("handle") for m in team.get("members", []) if m and m.get("handle")))
        w = f"{team.get('handle')} ({members})"
      candidates.append({
        "sub": sub,
        "credit": credit,
        "isLead": is_lead,
        "sliceCandidate": slice_candidate,
        "participant": w,
      })

    # Keep max sliceCandidate per participant
    best_by_participant = {}
    for c in candidates:
      p = c["participant"]
      if p not in best_by_participant or c["sliceCandidate"] > best_by_participant[p]["sliceCandidate"]:
        best_by_participant[p] = c

    deduped = list(best_by_participant.values())

    # Compute duplicate count based on deduped set (sum of credits; lead bonus handled later)
    for c in deduped:
      num_dups += c["credit"]

    if num_dups == 0:
      continue

    base_shares = 10 if is_high(finding) else 3
    finding["shares"] = base_shares * BASE**(num_dups - 1) / num_dups
    finding["dups"] = num_dups
    total_shares += finding["shares"] * (num_dups + 0.3)
    # Persist deduped candidates for later use when emitting records
    finding["_deduped"] = deduped

  # Summarise results for each handle
  for finding in findings:
    if "shares" not in finding:
      continue

    # Use previously computed per-participant deduped list
    deduped = list(finding.get("_deduped") or [])
    # Sort: lead first, then by shares contribution
    deduped.sort(key=lambda c: (not c["isLead"], -c["sliceCandidate"], c["sub"].get("number") or 0))

    for idx, c in enumerate(deduped):
      submission = c["sub"]
      credit = c["credit"]
      submission["sliceCredit"] = 1.3 if idx == 0 and c["isLead"] else credit
      submission["shares"] = finding["shares"] * submission["sliceCredit"]

      w = c["participant"]
      if not w in ws:
        ws[w] = { "rank": 0, "handle": w }
        if ns.pot_size != None:
          ws[w]["payout"] = 0.0
        ws[w].update({ "findings": [], "findingsCount": 0 })

      rec = {
        "submissionId": "S-" + str(submission.get("number")),
        "findingId": "F-" + str(finding.get("number")),
        "isLeadSubmission": bool(c["isLead"]) and idx == 0,
        "severity": finding["severity"],
        "dups": finding["dups"],
        "sliceCredit": submission["sliceCredit"],
        "shares": submission["shares"],
      }
      ws[w]["findings"].append(rec)

      if ns.pot_size != None:
        rec["payout"] = ns.pot_size * 0.8 * rec["shares"] / total_shares
        ws[w]["payout"] += rec["payout"]

  for w in ws:
    ws[w]["findingsCount"] = len(ws[w]["findings"])
    ws[w]["findings"].sort(key=lambda r: -r["shares"]) 

  # Compute fractions (share of total) for non-payout ranking and for reference
  if total_shares > 0:
    for w in ws:
      ws[w]["fraction"] = sum(rec.get("shares", 0) for rec in ws[w]["findings"]) / total_shares

  # Build duplicate sets per finding to compute effective duplicate counts (sum of sliceCredits)
  dup_sets = {}
  for w in ws:
    for rec in ws[w]["findings"]:
      fid = rec.get("findingId")
      if fid not in dup_sets:
        dup_sets[fid] = { "findings": [] }
      dup_sets[fid]["findings"].append(rec)

  # Calculate bonuses
  gatherers, highest_gatherer_score = ([], 0)
  hunters, highest_hunter_score = ([], 0)
  num_highs = sum(finding["severity"] == "high" and "shares" in finding for finding in findings)
  num_mediums = sum(finding["severity"] == "medium" and "shares" in finding for finding in findings)

  for w in ws:
    gatherer_score = 0
    hunter_score = 0
    for f in ws[w]["findings"]:
      severity_score = 10 if is_high(f) else 3
      total_severity_findings = num_highs if is_high(f) else num_mediums
      # Only full-credit findings count towards TH/TG scoring, but partial credits affect duplicate count
      # (see https://docs.code4rena.com/awarding/incentive-model-and-awards#bonuses-for-top-competitors)
      if f["sliceCredit"] >= 1:
        gatherer_score += severity_score / total_severity_findings
        # Calculate effective number of duplicates including partial credits, remove lead bonus (0.3)
        fid = f.get("findingId")
        group = dup_sets.get(fid, {"findings": []})["findings"]
        effective_dups = sum(d.get("sliceCredit", 0) for d in group) - 0.3
        if effective_dups < 5:
          hunter_score += severity_score / max(effective_dups, 1)

    ws[w]["gathererScore"] = gatherer_score
    if gatherer_score > highest_gatherer_score:
      highest_gatherer_score = gatherer_score
      gatherers = [w]
    elif gatherer_score == highest_gatherer_score:
      gatherers.append(w)

    ws[w]["hunterScore"] = hunter_score
    if hunter_score > highest_hunter_score:
      highest_hunter_score = hunter_score
      hunters = [w]
    elif hunter_score == highest_hunter_score:
      hunters.append(w)

  bonus = ns.pot_size * 0.1 if ns.pot_size != None else 0.1
  for role, winners in [("hunter", hunters), ("gatherer", gatherers)]:
    for winner in winners:
      ws[winner][f"{role}Bonus"] = round(bonus / len(winners), 2)
      if ns.pot_size != None:
        ws[winner]["payout"] += bonus / len(winners)

  # Prepare and format warden summaries for output
  warden_summaries = [ws[w] for w in ws if (lambda w: ns.handle == None or ns.handle in w)(w)]
  if ns.pot_size != None:
    warden_summaries.sort(key=lambda r: -r["payout"])
  else: # should be the same
    warden_summaries.sort(key=lambda r: -(r["fraction"] * 0.8 + r.get("hunterBonus", 0) + r.get("gathererBonus", 0)))

  for i in range(len(warden_summaries)):
    warden_summaries[i]['rank'] = i + 1
    if ns.pot_size != None:
      warden_summaries[i]['payout'] = round(warden_summaries[i]['payout'], 2)
      for finding in warden_summaries[i]['findings']:
        finding['payout'] = round(finding['payout'], 2)
    # Round floats in findings to avoid precision artifacts in output
    for finding in warden_summaries[i]['findings']:
      if 'shares' in finding:
        finding['shares'] = round(finding['shares'], 6)
      if 'sliceCredit' in finding:
        finding['sliceCredit'] = round(finding['sliceCredit'], 6)
      if 'dups' in finding:
        finding['dups'] = round(finding['dups'], 6)

  return { "totalShares": total_shares, "payouts": warden_summaries }

ns = parser.parse_args(sys.argv[1:])

#
# Set the base if the -b, --base flag is provided
# or the csv_file if it is provided
#
def set_base(base):
  global BASE
  if base != None:
    BASE = float(base[0])
  else:
    BASE = 0.85

if ns.command == "payouts":
  set_base(ns.base)
  result = payout(ns)
elif ns.command == "open":
  result = open_cmd(ns)
else:
  parser.parse_args(["--help"])

result["note"] = ("This tool only calculates shares for Highs and Mediums. " +
                  "It does not take into account QA reports.")

## Prints the records as valid JSON
print(json.dumps(result, indent=2))
